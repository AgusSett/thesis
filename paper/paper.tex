\documentclass[]{report}
\usepackage[a4paper]{geometry}
\usepackage[spanish]{babel}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{ebproof}
\usepackage{agda}
\usepackage{newunicodechar}
\newunicodechar{≡}{\ensuremath{\mathnormal\equiv}}
\newunicodechar{∀}{\ensuremath{\mathnormal\forall}}
\newunicodechar{⇒}{\ensuremath{\mathnormal\Rightarrow}}
\newunicodechar{⊤}{\ensuremath{\mathnormal\top}}
\newunicodechar{₁}{\ensuremath{{}_1}}
\newunicodechar{₂}{\ensuremath{{}_2}}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[style=alphabetic]{biblatex}
\addbibresource{references.bib}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark,calligraphy}
\usetikzlibrary{arrows.meta,positioning,matrix}
\setlength{\headheight}{13.07225pt}

%opening
\title{Formalización de Sistema I con tipo Top}
\author{Agustin Settimo}

\begin{document}
	
	\maketitle
	\tableofcontents
	
	\begin{abstract}
		
	\end{abstract}
	
	\chapter{Introducción}
	Los sistemas de tipos permiten especificar el comportamiento de los programas.
	Por ejemplo, el tipo $(A \times B) \rightarrow C$ representa los programas que permiten obtener un valor de tipo $C$ a partir de un par de valores de tipo $A$ y $B$. Por otro lado, el tipo $A \rightarrow B \rightarrow C$ representa los programas que primero acepta un valor de tipo $A$, luego uno de tipo $B$ y finalmente devuelve un valor de tipo $C$.
	Estas dos especificaciones son distintas en su forma pero parecen tener el mismo significado semántico.Esto se debe
	al isomorfismo de Curry.
	Debido a la estrecha relación entre la programación y la lógica, se observa un comportamiento análogo cuando se trabaja con pruebas.
	Por ejemplo, una prueba de $(A \wedge B) \Rightarrow C$ no constituye una prueba de $A \Rightarrow B \Rightarrow C$, y viceversa, a pesar de que ambas tienen el mismo significado.
	
	\section{Motivación}
	
	Tanto los sistemas de tipos como los sistemas de pruebas distinguen elementos que tienen diferente forma aunque tengan el mismo significado, como pueden ser las pruebas de las conjunciones $A \wedge B$ y $B \wedge A$, por lo cual una prueba de una no constituye una prueba de la otra, a pesar de que se puede demostrar mediante la existencia de un isomorfismo que dichas proposiciones son equivalentes.
	
	Programas con tipos isomorfos representan el mismo tipo de problema, por lo que tratarlos como si fueran idénticos tiene aplicaciones interesantes.
	Por ejemplo, desde el punto de vista de los programas, nos permite construir expresiones de formas que antes eran erróneas, un término $f : (A \wedge B) \Rightarrow C$ puede ser combinado como $f \langle a, b \rangle$ ó $f \: a \: b$, es decir, es posible sortear cierta rigidez impuesta por el sistema de tipos.
	Por otro lado, los isomorfismos hacen que las pruebas sean más naturales, por ejemplo, para probar, $A \wedge (A \Rightarrow  B) \Rightarrow B$, deberíamos primero introducir la hipótesis $A \wedge A \Rightarrow  B$, y luego descomponerla en $A$ y $A \Rightarrow B$, en cambio, utilizando el isomorfismo de curry transformamos el objetivo en $(A \Rightarrow A \Rightarrow  B) \Rightarrow B$ y luego introducimos directamente las hipótesis $A$ y $A \Rightarrow B$.
	
	\section{Estado del arte}
	\subsection{Isomorfismos de tipos}
	
	Para poder abstraernos de la forma y centrarnos en el significado de los programas, es necesario establecer cuáles son las formas que son combinables y cómo combinarlas.
	Para ello nos valdremos de la noción de isomorfismo entre tipos.
	El primer paso es considerar dos proposiciones $A$ y $B$ como isomorfas si existen dos pruebas $A \Rightarrow B$ y $B \Rightarrow A$, tal que al componerlas, en ambos sentidos, obtenemos como resultado la identidad.
	Di Cosmo \cite{MSCSSurvey05} caracterizó los conjuntos mínimos de isomorfismos que permiten construir todos los demás a partir de reglas de congruencia y transitividad.
	Se escribe $A \equiv B$ cuando $A$ y $B$ sean isomorfos.
	
	\begin{table}[ht!]
		\centering
		\begin{minipage}{0.7\linewidth}
			\begin{enumerate}
				\item[swap] $A \Rightarrow (B \Rightarrow  C) \equiv B \Rightarrow (A \Rightarrow  C)$ \tikzmark{swap}
				
				\item $A \times B \equiv B \times A$ \tikzmark{topTh1xT}
				\item $A \times (B \times C) \equiv (A \times B) \times C$
				\item $(A \times B) \Rightarrow C \equiv A \Rightarrow (B \Rightarrow C)$
				\item $A \Rightarrow (B \times C) \equiv (A \Rightarrow B) \times (A \Rightarrow C)$
				
				\item $A \times \textbf{T} \equiv A$
				\item $A \Rightarrow \textbf{T} \equiv \textbf{T}$
				\item $\textbf{T} \Rightarrow A \equiv A$ \tikzmark{botTh1xT}
				
				\item $\forall X. \forall Y. A \equiv \forall Y. \forall X. A$ \tikzmark{topTh2}
				\item $\forall X.A \equiv \forall Y.A [Y/X]$
				\item $\forall X. (A \Rightarrow B) \equiv A \Rightarrow \forall X.B]$ \tikzmark{botTh2}
				\item $\forall X. (A \times B) \equiv \forall X.A \times \forall X.B$
				\item $\forall X.\textbf{T} \equiv \textbf{T}$ \tikzmark{botTh2xT}
				
				\item[split] $\forall X. (A \times B) \equiv \forall X \forall Y.A \times \forall X. (B [Y/X])$ \tikzmark{split}
				
			\end{enumerate}
	
			\begin{tikzpicture}[overlay, remember picture]
				\draw [decoration={calligraphic brace,amplitude=0.4em},very thick,decorate]
				let \p1=(pic cs:swap) in
				(19em, \y1+1em) -- node[right=0.6em] {$Th^1$} (19em, \y1-0.6em);
				
				\draw [decoration={calligraphic brace,amplitude=0.4em},very thick,decorate]
				let \p1=(pic cs:topTh1xT), \p2=(pic cs:botTh1xT) in
				(19em, \y1+0.8em) -- node[right=0.6em] {$Th^1_{\times T}$} (19em, \y2);
				
				\draw [decoration={calligraphic brace,amplitude=0.4em},very thick,decorate]
				let \p1=(pic cs:topTh2), \p2=(pic cs:botTh2) in
				(15em, \y1+0.8em) -- node[right=0.6em] {+ swap = $Th^2$} (15em, \y2);
				
				\draw [decoration={calligraphic brace,amplitude=0.4em},very thick,decorate]
				let \p1=(pic cs:topTh1xT), \p2=(pic cs:botTh2xT) in
				(23em, \y1+0.8em) -- node[right=0.6em] {$Th^2_{\times T}$} (23em, \y2);
				
				\draw [decoration={calligraphic brace,amplitude=0.4em},very thick,decorate]
				let \p1=(pic cs:topTh1xT), \p2=(pic cs:split) in
				(27em, \y1+0.8em) -- node[right=0.6em] {$-10, 11 = Th^{ML}$} (27em, \y2);
			\end{tikzpicture}
		\end{minipage}
		\caption{Isomorfismos de tipo en cálculo lambda tipado}
	\end{table}
	
	\subsection{Sistema I}
	Sistema I \cite{system-i} es un lenguaje de pruebas para el fragmento de la lógica con $\Rightarrow$ y $\wedge$, que se corresponde con lambda cálculo simplemente tipado con pares.
	El conjunto de isomorfismos que abarca este sistema es denominado $Th^1_\times$.
	El primer paso para la internalización de los isomorfismos es la definición de una nueva regla de tipado llamada $(\equiv)$
	
	\[ \frac{A \equiv B \quad \Gamma \vdash r:A}{\Gamma \vdash r:B} (\equiv) \]
	
	Esta regla permite emplear términos $r:A$ en cualquier lugar que se necesite un cualquier tipo isomorfo a $A$. Por ejemplo:
	
	\begin{prooftree*}
		\hypo{ \Gamma\vdash r: A }
		\infer1[($\Rightarrow_i$)]{ \Gamma\vdash \lambda x.r: C \Rightarrow A }
		\hypo{ \Gamma\vdash s: B }
		\infer1[($\Rightarrow_i$)]{ \Gamma\vdash \lambda x.s: C \Rightarrow B }
		\infer2[($\times_i$)]{ \Gamma\vdash \langle \lambda x.r, \lambda x.s \rangle : (C \Rightarrow A) \times (C \Rightarrow B) }
		\infer1[($\equiv$)]{ \Gamma\vdash \langle \lambda x.r, \lambda x.s \rangle : C \Rightarrow (A \times B) }
		\hypo{ \Gamma\vdash t: C }
		\infer2[($\Rightarrow_e$)]{ \Gamma\vdash \langle \lambda x.r, \lambda x.s \rangle \; t : A \times B }
	\end{prooftree*}
	
	La introducción de una relación de equivalencia en el nivel de los tipos necesariamente deberá tener consecuencias en el nivel de los términos.
	Con las reglas de reducción usuales, un término como el del ejemplo anterior estaría en forma normal.
	Entonces el siguiente paso es extender la relación de reducción con	isomorfismos a nivel términos.
	Estas nuevas reglas permiten desatascar los términos que tipamos usando $(\equiv)$.
	Por ejemplo, $\langle r, s \rangle \; t \leftrightarrows \langle r t, s t \rangle$
	Es importante destacar que dependiendo del conjunto de isomorfismos de términos que se incluyen se obtienen sistemas de cálculo con distintas propiedades, incluir demasiados puede desencadenar en no terminación, y si se incluyen pocas pueden aparecer eliminaciones en formas normales.
	
	Se presentan las reglas propuestas por Sistema I, las cuales fueron elegidas con el objetivo de obtener un cálculo fuertemente normalizante y consistente:
	
	\begin{align}
		\langle r,s \rangle &\rightleftarrows \langle s,r \rangle \tag{\textsc{comm}} \\
		\langle r, \langle s,t \rangle \rangle &\rightleftarrows \langle \langle r, s \rangle, t \rangle \tag{\textsc{asso}} \\
		\lambda x^A \langle r,s \rangle &\rightleftarrows \langle \lambda x^A.r, \lambda x^A.s \rangle \tag{$\textsc{dist}_{\lambda}$} \\
		\langle r,s \rangle t &\rightleftarrows \langle r t, s t \rangle \tag{$\textsc{dist}_{app}$} \\
		r \langle s, t \rangle &\rightleftarrows r s t \tag{\textsc{curry}}
	\end{align}
	
	\begin{figure}[ht!]
		\centering
		\begin{prooftree}
			\infer0[($ax$)]{ \Gamma, x:A \vdash x:A }
		\end{prooftree}
		\quad
		\begin{prooftree}
			\hypo{A \equiv B}
			\hypo{\Gamma \vdash r:A}
			\infer2[($\equiv$)]{ \Gamma \vdash r:B }
		\end{prooftree}
		\vspace{1em}
		\\
		\begin{prooftree}
			\hypo{\Gamma, x:A \vdash r:B}
			\infer1[($\Rightarrow_i$)]{ \Gamma \vdash \lambda x.r : A \Rightarrow B }
		\end{prooftree}
		\quad
		\begin{prooftree}
			\hypo{\Gamma \vdash \lambda x.r : A \Rightarrow B}
			\hypo{\Gamma \vdash s:A}
			\infer2[($\Rightarrow_e$)]{ \Gamma \vdash rs : B }
		\end{prooftree}
		\vspace{1em}
		\\
		\begin{prooftree}
			\hypo{\Gamma \vdash r:A}
			\hypo{\Gamma \vdash s:B}
			\infer2[($\times_i$)]{ \Gamma \vdash \langle r, s \rangle : A \times B }
		\end{prooftree}
		\quad
		\begin{prooftree}
			\hypo{\Gamma \vdash \langle r, s \rangle : A \times B}
			\infer1[($\times_e$)]{ \Gamma \vdash \pi_A\langle r, s \rangle : A }
		\end{prooftree}
		\caption{Reglas de tipado}
	\end{figure}
	
	% mostrar isos de termino y reglas de tipado
	
	El siguiente paso surge de notar que las nuevas reglas introducen problemas en la reducción clásica.
	Por un lado, las reglas de proyección $\pi_1$ y $\pi_2$ indexan el par a través de la posición de los elementos, pero el isomorfismo $\textsc{comm}$ permite cambiar el orden de un par, permitiendo así proyectar cualquiera de los dos elementos:
	
	\begin{align*}
		\langle r, s \rangle& \hookrightarrow_{\pi_1} r \\
		\langle r, s \rangle& \leftrightarrows_{\textsc{comm}} \langle s, r \rangle \hookrightarrow_{\pi_1} s
	\end{align*}
	
	Esto supone un problema para la preservación de tipos.
	La solución es indexar el elemento a través de su tipo, por lo que se define una nueva regla:
	
	\[ \text{si} \; r:A \quad \pi_A \langle r, s \rangle \hookrightarrow_{\pi} r \]
	
	Una consecuencia importante es que esta regla introduce no determinismo en el cálculo.
	Si $r$ y $s$ tienen tipo $A$, entonces $\pi_A \langle r, s \rangle$ puede reducir indistintamente a cualesquiera de los dos términos.
	
	% explica como se pueden encodear los pares
	
	La $\beta$-reducción clásica también entra en conflicto con la preservación de tipos.
	Por ejemplo, el término $(\lambda x^A . \lambda y^B . r)ts$ donde $t:B$ y $s:A$ está bien tipado, pero para reducirlo correctamente primero se deben aplicar equivalencias para intercambiar el orden de $t$ y $s$.
	Sin embargo, no hay nada que impida aplicar la regla $\beta$, lo cual podría terminar reduciendo a un tipo equivocado.
	Para solucionar esto, se modifica la $\beta$-reducción:
	
	\[ \text{si} \; s:A \quad (\lambda x^A.r) s \hookrightarrow_{\beta} r[s/x] \]
	
	Por último se define la relación de reducción módulo isomorfismos:
	
	\[ \rightsquigarrow \; = \; \leftrightarrows^* \circ \hookrightarrow \circ \leftrightarrows^* \]
	
	Es decir, que primero los términos son transformados a una forma equivalente donde sea posible aplicar las eliminaciones, y luego se aplican las $\beta$-reducciones y proyecciones.
	
	
	\subsection{SIP}
	Sistema I Polimórfico \cite{sip}, es una extensión que agrega polimorfismos a Sistema I, tomas algunos de los isomorfismos del conjunto axiomático $Th^2_\times$.
	Desde el punto de vista de la lógica, agrega el conector $\forall$, y en cuanto a la programación, agrega la gramática
	de tipos y términos correspondientes a Sistema F con pares.
	
	El trabajo presenta la técnica utilizada para obtener las equivalencias de términos inducidas por un isomorfismo de tipos.
	Dado un isomorfismo de tipos con dos constructores involucrados, se construyen términos aplicando de diferentes maneras las reglas de tipado relacionadas con los constructores.
	Para cualquier par de conectivas, tenemos cuatro posibles formas de combinar sus reglas de tipado: introducción-introducción, introducción-eliminación, eliminación-introducción, y eliminación-eliminación.
	Cada una induce una equivalencia, y cada lado de la equivalencia estará dado por el orden en que se apliquen las conectivas.
	%TODO: agregar un ejemplo
	

	\subsection{$\lambda^+$}
	$\lambda^+$ \cite{lambda-plus} es una implementación en Haskell de una extensión que agrega naturales y recursión general a Sistema I.
	Este trabajo muestra las dificultades de implementar un sistema de tipos módulo isomorfismos y las aplicaciones prácticas de un lenguaje de programación con dichas capacidades.
	Por ejemplo, utilizando $\textsc{curry}$, $\textsc{comm}$ y $\textsc{asso}$ es posible aplicar parcialmente los argumentos de una función en cualquier orden.
	
	Un detalle interesante es la adición de una nueva regla llamada $\textsc{slit}$ que no surge de ningún isomorfismo, sino que es necesaria para reducir términos que de otra forma quedarían atascados.
	Esto muestra que desde el punto de vista de la implementación, puede ser necesario agregar nuevas equivalencias de términos para lograr reducir a una forma normal.
	% explicar que introduce el split, es necesario agregar o eliminar isos para lograr la reduccion 
	
	
	\section{Estructura del trabajo}
	
	\chapter{Preliminares}
	
	\section{Tipos dependientes}
	El ejemplo más común de un tipo dependiente que se suele presentar, es el de los vectores de largo $n$:
	
	\begin{lstlisting}[mathescape, language=Haskell]
	data Vec A (n: $\mathbb{N}$) where
	  nil : Vec A 0
	  cons : A $\rightarrow$ Vec A n $\rightarrow$ Vec A (suc n)
	\end{lstlisting}
	
	Es importante notar que el segundo parámetro de \verb|Vec| no es el tipo $\mathbb{N}$, sino un elemento de tipo $\mathbb{N}$, por ejemplo, \verb|Vec| $\mathbb{N}$ \verb|3| representa el tipo de los vectores de naturales de largo 3.
	Esto quiere decir que dentro de los tipos pueden aparecer términos, lo cual permite escribir programas más precisos y seguros.
	
	Para entender la practicidad de un sistema de tipos con dicho nivel de especificidad, se comparan los siguientes programas:
	\begin{lstlisting}[mathescape, language=Haskell]
	zeroesL : $\mathbb{N}$ $\rightarrow$ List $\mathbb{N}$
	zeroesL 0       = []
	zeroesL (suc n) = 0 :: (zeroesL n)
	
	zeroesV : (n: $\mathbb{N}$) $\rightarrow$ Vec $\mathbb{N}$ n
	zeroesV 0       = nil
	zeroesV (suc n) = cons 0 (zeroesV n)
	\end{lstlisting}
	
	Notar que el tipo de $zeroesV$ es dependiente e introduce una variable.
	Si bien la implementación dada para $zeroesL$ es correcta, nada impediría simplemente retornar siempre \verb|[]|, por el contrario, en el caso \verb|(suc n)| de $zeroesV$ se estaría cometiendo un error de tipo, ya que se espera un término de tipo \verb|Vec| $\mathbb{N}$ \verb|(suc n)| y la lista vaciá tiene tipo \verb|Vec| $\mathbb{N}$ \verb|0|.
	De cierta forma el tipo está guiando la implementación, impidiendo retornar un vector de largo incorrecto.
	
	Otra compasión que pone en evidencia la correctitud de los lenguajes con tipos dependientes, es la diferencia entre el operador de indexación de listas y vectores.
	
	\begin{lstlisting}[mathescape, language=Haskell]
	_!!_ : List A $\rightarrow$ $\mathbb{N}$ $\rightarrow$ Maybe A
	[] !! n              = nothing
	(x :: xs) !! zero    = just x
	(x :: xs) !! (suc n) = xs !! n
	\end{lstlisting}
	
	Antes de la implementación de vectores es necesario definir el tipo de datos de los conjuntos finitos:
	
	\begin{lstlisting}[mathescape, language=Haskell]
	data Fin $\mathbb{N}$ where
	  zero : Fin (suc n)
	  suc  : Fin n $\rightarrow$ Fin (suc n)
	\end{lstlisting}
	
	Cada tipo \verb|Fin n| está habitado por $n$ elementos, estos serían, el elemento \verb|zero| más el constructor \verb|suc| combinado con los $n-1$ elementos de \verb|Fin (n-1)|. Notar que \verb|Fin 0| es un conjunto vacío, es decir que no existe ningún término con dicho tipo.
	Luego, un vector de largo $n$ estará indexado por los elementos del conjunto \verb|Fin n|, que tiene cardinalidad exactamente igual a $n$.
	
	\begin{lstlisting}[mathescape, language=Haskell]
	_!!v_ : {n : $\mathbb{N}$} $\rightarrow$ Vec A n $\rightarrow$ Fin n $\rightarrow$ A
	(cons x v) !!v zero  = x
	(cons x v) !!v suc i = v !!v i
	\end{lstlisting}
	
	En la implementación ya no aparece el caso \verb|nil|, porque eso implicaría que $n = 0$, pero el tipo está forzando al argumento \verb|Fin| a tener la misma cantidad de elementos que el vector, por lo que dicho argumento debería ser un elemento de \verb|Fin 0|, el cual es un conjunto vació. Esto es lo que se denomina un ``patrón absurdo''.
	
	Utilizando estos tipos es imposible tratar de acceder a un elemento que no esté en el rango del vector.
	Lo más interesante es que los invariantes son verificados estáticamente por el sistema de tipos en tiempo de compilación. Un término que trata de romper alguna invariante, está mal tipado, por lo que ni siquiera es posible construirlo.
		
	
	\subsection{Lambda cube}
	% mostrar los distintos tipos de calculos que surgen en cada dimension
	
	\begin{figure}[h!]
		\centering
	\begin{tikzpicture}
		\matrix (m) [matrix of math nodes,
		row sep=3em, column sep=2.4em,
		text height=1.5ex,
		text depth=0.25ex]{
						& \lambda\omega             &              & \lambda C					  \\
			\lambda 2   &                           & \lambda\Pi 2                                \\
						& \lambda\underline{\omega} &              & \lambda\Pi\underline{\omega} \\
			\lambda{\to}&                           & \lambda\Pi 								  \\
		};
		\path[-{Latex[length=2.5mm, width=1.5mm]}]
		(m-1-2) edge (m-1-4)
		(m-2-1) edge (m-2-3)
				edge (m-1-2)
		(m-3-2) edge (m-1-2)
				edge (m-3-4)
		(m-4-1) edge (m-2-1)
				edge (m-3-2)
				edge (m-4-3)
		(m-3-4) edge (m-1-4)
		(m-2-3) edge (m-1-4)
		(m-4-3) edge (m-3-4)
				edge (m-2-3);
	\end{tikzpicture}
	\caption{Lambda cube}
	\end{figure}
	
	\section{Propositions as Types}
	El paradigma de ``proposiciones como tipos'' describe la correspondencia entre la lógica y los lenguajes de programación.
	Básicamente, dice que para a cada proposición en la lógica le corresponde un tipo, y viceversa.
	De hecho, esta relación es más profunda, ya que a cada prueba de una proposición dada, le corresponde un programa del tipo correspondiente, y viceversa.
	Es decir, ``pruebas como programas''.
	Incluso es más profunda aún, en el sentido de que para cada forma de simplificar una prueba, existe una forma correspondiente de evaluar un programa, y viceversa. Por lo que tenemos, ``simplificación de pruebas como evaluación de programas''.
	
	Tal como lo explica Wadler en su artículo \cite{pas}, no se trata de una simple biyección entre proposiciones y tipos, sino un verdadero isomorfismo que preserva la compleja estructura de pruebas y programas, simplificaciones y evaluaciones.
	
	Este principio surge de las observaciones realizadas por Curry sobre la lógica proposicional, y más tarde extendidas por Howard a la lógica de predicados.
	La clave de esta extensión es la introducción de los tipos dependientes para representar los predicados y cuantificadores en la lógica de predicados.

	Las correspondencias que surgen de esta interpretación pueden resumirse de la siguiente forma:
	
	\begin{itemize}
		\item La conjunción $A \wedge B$ corresponde al par $A \times B$.
		Una prueba de la proposición $A \wedge B$ consiste de una prueba de $A$ y una prueba de $B$.
		
		\item La disyunción $A \vee B$ corresponde la suma disyunta $A + B$.
		Una prueba de la proposición $A \vee B$ consiste de una prueba de $A$ o una prueba de $B$.
		
		\item La implicación $A \Rightarrow B$ corresponde al espacio de funciones $A \rightarrow B$.
		Una prueba de la proposición $A \Rightarrow B$ consiste de una función que dada una prueba de $A$ devuelve una prueba de $B$.
		
		\item El cuantificador existencial $\exists x:A.B$ corresponde al tipo $\Sigma x:A.B$.
		Básicamente, esto es una familia de tipo indexada por $a : A$ donde a cada término $a$ le corresponde un tipo $B(a)$.
		Los elementos canónicos de $\Sigma x:A.B$ son pares dependientes $\langle a, b \rangle$ donde $a:A$ y $b:B(a)$.
		Cuando $B(a)$ es una función constante, este tipo es equivalente al producto cartesiano $A \times B$.
		
		
		\item El cuantificador universal $\forall x:A.B$ corresponde $\Pi x:A.B$, al igual que para el tipo $\Sigma$, $B$ está indexado por los términos de tipo $A$.
		Los elementos canónicos de $\Pi x:A.B$ son funciones dependientes $a \rightarrow B(a)$.
		Cuando $B(a)$ es una función constante, el tipo $\Pi$ es equivalente al tipo de las funciones ordinarias $A \rightarrow B$.
	\end{itemize}
	

	\section{Intuitionistic Type Theory}
	La teoría de tipos intuicionista, también llamada teoría de tipos de Martin-Löf propone un sistema lógico formal y los fundamentos filosóficos para las matemáticas constructivas.
	
	Directamente influenciado por las ideas de Howard, Martin-Löf se basó en el principio de proposiciones como tipos y el constructivismo matemático.
	Este constructivismo requiere que las pruebas contengan un ``testigo'', una prueba de una proposición dada es un programa, por lo tanto, las proposiciones son verdaderas cuando su tipo está habitado por algún término.
	Las pruebas son términos que atestiguan la veracidad del teorema, y pueden ser manipulados como cualquier otro término del lenguaje.
	
	Esto tiene algunas consecuencias interesantes, por ejemplo, en la lógica clásica, a las proposiciones se les asigna valores de verdad sin importar si existe evidencia directa de que sea verdadera o falsa.
	Esto es lo que comúnmente se denomina ``principio del tercero excluido'', ya que excluye la posibilidad de un tercer valor distinto de verdadero o falso.
	Por lo tanto, no es posible probar $A \vee \not A$ en la lógica intuicionista.

	
	Se puede pensar a la teoría de tipos intuicionista como un lenguaje de programación funcional donde el sistema de tipos es tan rico que prácticamente cualquier propiedad concebible de un programa puede expresarse como un tipo.
	Todas las funciones de este lenguaje deben ser totales y computables, por lo que todos los programas deben necesariamente cumplir con la propiedad de terminación.
	
	Filosófica y prácticamente, esta teoría de tipos es un marco fundamental donde las matemáticas constructivas y la programación son, en un sentido profundo, lo mismo.
	
	
	\section{Agda}
	Agda es un lenguaje de programación con tipos dependientes, desarrollado por la Universidad de
	Chalmers (Suiza).
	Debido al paradigma de las proposiciones como tipos, Agda también funciona como un asistente de pruebas.
	A diferencia de otros asistentes, como Coq, carece de un sistema de tácticas, por lo que las pruebas son escritas en un estilo de programación funcional, de hecho su sintaxis es similar a la de Haskell.

	Agda es un lenguaje total, es decir que todos los programas deben terminar, y todos los posibles casos de un pattern matching deben ser cubiertos, de otro modo la lógica sería inconsistente.
	Por este motivo, no todas las funciones recursivas están permitidas, Agda poseé un mecanismo de comprobación de terminación que acepta aquellas funciones que puede probar mecánicamente su terminación.
	
	
	\section{Indices de DeBruijn}
	Generalmente, los términos en cálculo lambda se presentan utilizando letras para nombrar las variables, por ejemplo:
	\[ \lambda z. (\lambda y. y (\lambda x. x)) (\lambda x. z x) \]
	Esta forma de representación permite ver a simple vista cuáles variables están ligadas y a que abstracción pertenecen, también suelen utilizarse palabras para dar nombres más descriptivos a las variables.
	Se puede notar que la forma de escribir un término no es única, ya que es posible cambiar los nombres de algunas variables sin alterar su significado, por ejemplo, $\lambda c. (\lambda b. b (\lambda d. d)) (\lambda a. c a)$ es equivalente al del ejemplo anterior.
	Cuando esto ocurre se dice que los términos son $\alpha$-equivalentes.
	
	El principal problema de esta representación es que para implementar la $\beta$-reducción, se debe tener cuidado de no capturar una variable libre cuando se substituye un término en el cuerpo de una abstracción, en caso de que eso ocurra, se renombra la variable capturada con un nuevo nombre fresco.
	En el siguiente ejemplo la variable $x$ es renombrada a $z$ para evitar la captura:
	\[ (\lambda y. (\lambda x. x y)) x \rightarrow_{\beta} (\lambda x. x y)[y := x] \rightarrow \lambda z. z x \]
	
	Utilizar variables con nombres hace que la implementación se vuelva más engorrosa e ineficiente.
	Una alternativa más adecuada es la representación de DeBruijn, que reemplaza los nombres por números naturales llamados \textit{índices de DeBruijn}.
	Por ejemplo, la forma de escribir el término del primer ejemplo con índices es la siguiente:
	
	\[ \textcolor{red}{\lambda} (\textcolor{blue}{\lambda\; 0} \; (\textcolor{orange}{\lambda\; 0})) (\textcolor{green}{\lambda}\; \textcolor{red}{1} \; \textcolor{green}{0}) \]

	Los índices indican cuantas abstracciones se deben ``saltar'' para llegar a la que está ligando la variable.
	Los nombres de las ligaduras ya no son necesarios, por lo que la escritura se simplifica.
	Además, los términos tienen una única representación, es decir, no es necesario tener en cuenta las $\alpha$-equivalencias.
	
	
	\section{Substituciones explícitas}
	Utilizando la representación de DeBruijn, las sustituciones son simplemente mapas de números naturales a términos, es decir que pueden interpretarse como una secuencia infinita de términos.
	Por ejemplo:
	\[ (0\; 1\; 3)\{0\mapsto a, 1\mapsto b, i+2\mapsto i\} \rightarrow a\; b\; 1 \]
	
	Si se quisiera definir la $\beta$ reducción utilizando esta notación, una primera definición sería:
	
	\[ (\lambda a)b \rightarrow_{\beta} a \{ 0 \mapsto b, i+1\mapsto i+1 \} \]
	
	El problema es que al eliminar un $\lambda$ todas las variables libres de $a$ quedaran desfasadas, por lo que se les debe restar uno:
	
	\[ (\lambda a)b \rightarrow_{\beta} a \{ 0 \mapsto b, i+1\mapsto i \} \]
	
	El siguiente problema surge al intentar empujar la substitución dentro de una abstracción, en dicho caso, se debe evitar reemplazar el índice 0 por $b$:
	
	\[ (\lambda c)\{ 0 \mapsto b, i+1\mapsto i \} = \lambda c \{ 0 \mapsto 0, 1 \mapsto b, i+2\mapsto i+1 \} \]
	
	Un último problema puede presentarse si $b$ tiene variables libres, para evitar que estas sean capturadas por el $\lambda$ de $c$ se les debe sumar uno:
	\[ (\lambda c)\{ 0 \mapsto b, i+1\mapsto i \} = \lambda c \{ 0 \mapsto 0, 1 \mapsto b \{ i \mapsto i+1 \}, i+2\mapsto i+1 \} \]
	
	El siguiente ejemplo muestra la forma correcta de realizar una $\beta$ reducción:
	
	\[
	(\textcolor{red}{\lambda}\; \lambda\; 3\; \textcolor{red}{1}\; (\textcolor{green}{\lambda\; 0}\; \textcolor{red}{2}))\; (\textcolor{blue}{\lambda}\; 4\; \textcolor{blue}{0})
	\rightarrow_\beta
	\lambda \; 2\; (\textcolor{blue}{\lambda}\; 5\; \textcolor{blue}{0})\; (\textcolor{green}{\lambda\; 0}\; (\textcolor{blue}{\lambda}\; 6\; \textcolor{blue}{0}))\;
	\]
	
	Notar como las variables libres del cuerpo de la abstracción se disminuyen en uno, las variables libres del argumento aumentan en uno por cada $\lambda$ que atraviesan, y las variables ligadas quedan intactas.
	
	
	En \cite{explicit_subs} se presenta el álgebra $\sigma$ y los cuatro operadores que permiten construir estas secuencias.

	\begin{itemize}
		\item $id$ es la substitución identidad $\{i \mapsto i\}$.
		\item $\uparrow$ es el operador shift, y suma uno a cada índice $\{i \mapsto i+1\}$.
		\item $a \cdot s$ es la concatenación del término $a$ a la substitución $s$, $\{0 \mapsto a, i+1 \mapsto s(i)\}$. Por ejemplo $a \cdot s = \{ 0 \mapsto a, 1 \mapsto 0, 2 \mapsto 1, 3 \mapsto 2, \dots \} $
		\item $s \circ t$ corresponde a la composición de substituciones, donde primero se aplica $s$ y luego $t$.
	\end{itemize}
	
	Dada una substitución $s$, se escribe $a[s]$ para denotar la aplicación de la substitución sobre el término $a$.
	\begin{align*}
		n[s] &= s(n) \\
		(a\; b)[s] &= a[s]\; b[s] \\
		(\lambda a)[s] &= \lambda a[1 \cdot (s \; \circ \uparrow)]
	\end{align*}
	
	Finalmente, la $\beta$ reducción se define como:
	\[ (\lambda a)b \rightarrow_{\beta} a[b \cdot id] \]
	
	El trabajo Abadi también presenta algunas propiedades algebraicas de los operadores:
	\begin{align*}
		0\; \cdot \uparrow &= id \\
		\uparrow \circ\; (a \cdot s) &= s \\
		0[s]\; \cdot (\uparrow \circ\; s) &= s \\
	\end{align*}
	
	
	\chapter{Aportes}
	\section{Formalización}
	\subsection{Tipos intrínsecos}
	Explicar la diferencia entre Curry style y Church style, y como nos ayuda tener tipos intrínsecos. Acá hay muchas referencias
	[https://plfa.github.io/DeBruijn/] (https://plfa.github.io/DeBruijn/).
	\subsection{STLC con pares y top}
	Formalizar tipos, términos y relación de reducción.
	Las substituciones están basadas en http://strictlypositive.org/ren-sub.pdf
	\subsection{Isomorfismo de términos}
	Explicar los isos nuevos que voy a agregar. Formalizar iso tipos y relación de iso.
	
	\subsection{Preservación de tipos}
	
	\section{Progreso}
	
	\subsection{Formas normales, neutrales y valores}
	\subsection{Relación de reducción}
	
	\section{Normalización Fuerte}
	
	\subsection{Prueba para STLC con pares y top}
	Primero mostrar la prueba sin isomorfismos. Puedo y extendiéndola de a poco como en el apunte de Beta Ziliani
	\subsection{Diferencias con la prueba de Girard}
	Capitulo 5.6 como referencia
	
	[https://www.ps.uni-saarland.de/~schaefer/thesis/draft-screen.pdf] 
	
	(https://www.ps.uni-saarland.de/~schaefer/thesis/draft-screen.pdf)
	\subsection{Prueba para Sistema I}
	\subsection{Well-founded \& Accessibility}
	Explicar que lo que hicimos en realidad es probar que la inducción sobre la relación de reducción está bien fundada
	[https://agdapad.quasicoherent.io/~AgdaPadova/html/transcript10.html]
	(https://agdapad.quasicoherent.io/~AgdaPadova/html/transcript10.html).
	Mostrar eval.
	
	\section{Conclusiones}
	
	
	\section{Trabajo futuro}
	\subsection{Inferencia de tipos}
	\subsection{Formalizar SIP}
	
	
	\printbibliography
	
\end{document}


